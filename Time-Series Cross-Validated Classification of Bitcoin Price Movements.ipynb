{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 31192,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "756_final_AA",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aiyman14/DACSS-756-Machine-Learning-for-Social-Science-/blob/main/Time-Series%20Cross-Validated%20Classification%20of%20Bitcoin%20Price%20Movements.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T22:18:34.731839Z",
          "iopub.execute_input": "2025-12-17T22:18:34.732189Z",
          "iopub.status.idle": "2025-12-17T22:18:34.737962Z",
          "shell.execute_reply.started": "2025-12-17T22:18:34.732152Z",
          "shell.execute_reply": "2025-12-17T22:18:34.737067Z"
        },
        "id": "xTzL5p7VLNSJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1) Motivation\n",
        "\n",
        "In this project, I use supervised machine learning to predict whether the price of Bitcoin will increase or decrease in the next hour based on historical market data. This is a binary classification problem where the target variable indicates whether the next hour’s closing price is higher than the current hour’s closing price. The data I am using is Yahoo Finances hourly data of bitcoin prices from December 20th, 2023, till the present day. There are some standard features in the dataset originally, but I constructed some more using information that is available up to the current hour, while the target represents a future outcome that is unknown at prediction time.\n",
        "\n",
        "This is a useful machine learning problem because short-term cryptocurrency price movements are noisy and fast-moving, making them hard to predict. On top of that they are influenced by complex interactions between momentum, volatility, and market behavior. While people can analyze charts or technical indicators manually, doing so consistently at an hourly frequency is time-consuming and would just be a very strenious task, and it becomes difficult to process many signals at once. Supervised learning models can evaluate these patterns systematically and detect subtle relationships across multiple features that are hard to track manually.\n",
        "\n",
        "Using machine learning also saves time and money compared to other methods. Once trained, a model can generate predictions quickly and consistently without requiring constant human intervention. This makes it especially useful for monitoring markets in real time or supporting automated decision-making systems. I think in general, this task fits the very basic description of supervised learning, using past data to make informed predictions about future outcomes. Even though cryptocurrency markets are challenging to forecast, and influenced by other real world factors not just past data, predicting short-term price direction from past data is a realistic way to apply supervised machine learning in this setting."
      ],
      "metadata": {
        "id": "OtJnbgIiLNSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yfinance --quiet\n",
        "\n",
        "import yfinance as yf"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T22:18:34.739283Z",
          "iopub.execute_input": "2025-12-17T22:18:34.739581Z",
          "iopub.status.idle": "2025-12-17T22:18:38.663106Z",
          "shell.execute_reply.started": "2025-12-17T22:18:34.739552Z",
          "shell.execute_reply": "2025-12-17T22:18:38.661782Z"
        },
        "id": "RO35VBnzLNSQ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "btc = yf.download(\n",
        "    tickers=\"BTC-USD\",\n",
        "    interval=\"60m\",\n",
        "    start=\"2023-12-20\",\n",
        "    progress=False\n",
        ")\n",
        "\n",
        "btc.head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T22:18:38.665257Z",
          "iopub.execute_input": "2025-12-17T22:18:38.665601Z",
          "iopub.status.idle": "2025-12-17T22:18:39.258945Z",
          "shell.execute_reply.started": "2025-12-17T22:18:38.665571Z",
          "shell.execute_reply": "2025-12-17T22:18:39.258061Z"
        },
        "id": "fqxi_7JdLNSR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "btc.info()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T22:18:39.259797Z",
          "iopub.execute_input": "2025-12-17T22:18:39.260029Z",
          "iopub.status.idle": "2025-12-17T22:18:39.272571Z",
          "shell.execute_reply.started": "2025-12-17T22:18:39.260011Z",
          "shell.execute_reply": "2025-12-17T22:18:39.271434Z"
        },
        "id": "iBUYo0lBLNSR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "btc.columns"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T22:18:39.274398Z",
          "iopub.execute_input": "2025-12-17T22:18:39.27505Z",
          "iopub.status.idle": "2025-12-17T22:18:39.294077Z",
          "shell.execute_reply.started": "2025-12-17T22:18:39.275023Z",
          "shell.execute_reply": "2025-12-17T22:18:39.292767Z"
        },
        "id": "cOMD92ehLNSR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "btc.index"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T22:18:39.295125Z",
          "iopub.execute_input": "2025-12-17T22:18:39.295467Z",
          "iopub.status.idle": "2025-12-17T22:18:39.314231Z",
          "shell.execute_reply.started": "2025-12-17T22:18:39.295443Z",
          "shell.execute_reply": "2025-12-17T22:18:39.313371Z"
        },
        "id": "ZD_zW-aNLNSR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "btc.to_csv(\"/kaggle/working/btc_hourly.csv\", index=False)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T22:18:39.315113Z",
          "iopub.execute_input": "2025-12-17T22:18:39.315408Z",
          "iopub.status.idle": "2025-12-17T22:18:39.536964Z",
          "shell.execute_reply.started": "2025-12-17T22:18:39.315381Z",
          "shell.execute_reply": "2025-12-17T22:18:39.536393Z"
        },
        "id": "3n-NX5KpLNSR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "problems with the initial data:\n",
        "\n",
        "The data did not include a proper datetime column, so there was no clear indication of when each hourly observation occurred. Since the entire goal of this project is to predict the next hour’s price movement using past information, having an accurate timestamp is essential.\n",
        "\n",
        "The dataset contained an extra first row with the value “BTC-USD\". This caused all columns to be read as strings instead of numeric values, which led to repeated type conversion issues when performing calculations."
      ],
      "metadata": {
        "id": "gwpptv3qLNSR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#flatten columns\n",
        "if isinstance(btc.columns, pd.MultiIndex):\n",
        "    btc.columns = btc.columns.get_level_values(-1)\n",
        "\n",
        "#rename columsn\n",
        "btc.columns = ['Close', 'High', 'Low', 'Open', 'Volume']\n",
        "\n",
        "#create copy of desired coluns\n",
        "btc = btc[['Close', 'High', 'Low', 'Open', 'Volume']].copy()\n",
        "\n",
        "#turn datetime into an actual columns\n",
        "btc.index.name = \"datetime\"\n",
        "btc = btc.reset_index()\n",
        "\n",
        "# make sure columns are numeric\n",
        "num_cols = ['Close', 'High', 'Low', 'Open', 'Volume']\n",
        "btc[num_cols] = btc[num_cols].astype(float)\n",
        "\n",
        "#save clean version\n",
        "btc.to_csv(\"btc_hourly_clean.csv\", index=False)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T22:18:39.5376Z",
          "iopub.execute_input": "2025-12-17T22:18:39.537826Z",
          "iopub.status.idle": "2025-12-17T22:18:39.759101Z",
          "shell.execute_reply.started": "2025-12-17T22:18:39.537808Z",
          "shell.execute_reply": "2025-12-17T22:18:39.758232Z"
        },
        "id": "9aSa8id2LNSR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "raw",
      "source": [
        "Here I try to fix some of the issues with the dataset so I can actually use it.\n",
        "\n",
        "Yahoo Finance often returns price data with extra labeling layers, so the I check for that and flatten the columns down to the actual price fields like Close, High, Low, Open, and Volume.\n",
        "\n",
        "Next, the datetime information is moved out of the index and into its own column, which makes it much easier to see the timing of each hourly observation and makes it easier to work with when creating features or saving the data. I also make sure all price and volume columns are truly numeric so I can use mathematical perations on them later.\n",
        "\n",
        "I then save the data as a new cleaned version that can be reproduced."
      ],
      "metadata": {
        "id": "BBu-mKQMLNSR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "btc.head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T22:18:39.760271Z",
          "iopub.execute_input": "2025-12-17T22:18:39.760886Z",
          "iopub.status.idle": "2025-12-17T22:18:39.772538Z",
          "shell.execute_reply.started": "2025-12-17T22:18:39.760855Z",
          "shell.execute_reply": "2025-12-17T22:18:39.771842Z"
        },
        "id": "bkCsxFX-LNSU"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "btc.info()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T22:18:39.77362Z",
          "iopub.execute_input": "2025-12-17T22:18:39.773949Z",
          "iopub.status.idle": "2025-12-17T22:18:39.796671Z",
          "shell.execute_reply.started": "2025-12-17T22:18:39.773927Z",
          "shell.execute_reply": "2025-12-17T22:18:39.795661Z"
        },
        "id": "trIC3VcELNSV"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#make sure datetime is in the right format\n",
        "btc['datetime'] = pd.to_datetime(btc['datetime'])\n",
        "\n",
        "#sort in chronological order\n",
        "btc = btc.sort_values('datetime').reset_index(drop=True)\n",
        "\n",
        "#check to make sure it worked\n",
        "btc.info()\n",
        "btc.head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T22:18:39.800451Z",
          "iopub.execute_input": "2025-12-17T22:18:39.800732Z",
          "iopub.status.idle": "2025-12-17T22:18:39.833676Z",
          "shell.execute_reply.started": "2025-12-17T22:18:39.800708Z",
          "shell.execute_reply": "2025-12-17T22:18:39.832772Z"
        },
        "id": "BT5Lri_ILNSV"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I am making sure the datetime column is actually usable. I am just forcing it into the proper pandas datetime format jus to make sure there are no issues later.  Then I sort by datetime so the rows are guaranteed to be in the right time order so I can use it later.Resetting the index just cleans up the row numbers after sorting."
      ],
      "metadata": {
        "id": "PzC2ZC9gLNSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  hourly return based on the closing price\n",
        "btc['return_1h'] = btc['Close'].pct_change()\n",
        "btc['target'] = (btc['return_1h'].shift(-1) > 0).astype(int)\n",
        "\n",
        "#label whether the next hour's price goes up or not\n",
        "btc = btc.iloc[:-1].copy()\n",
        "\n",
        "#check balance\n",
        "btc[['datetime', 'Close', 'return_1h', 'target']].head()\n",
        "btc['target'].value_counts(normalize=True)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T22:18:39.83463Z",
          "iopub.execute_input": "2025-12-17T22:18:39.834936Z",
          "iopub.status.idle": "2025-12-17T22:18:39.850236Z",
          "shell.execute_reply.started": "2025-12-17T22:18:39.834912Z",
          "shell.execute_reply": "2025-12-17T22:18:39.849385Z"
        },
        "id": "T8a2rrA_LNSV"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I am creating the target variable.\n",
        "\n",
        "I first calculate the hourly return using the percentage change in the closing price, which tells me how much the price moved from one hour to the next. Then I shift that return forward by one hour so that, for each row, the target reflects what happens in the next hour rather than the current one. If the next hour’s return is positive, I label it as 1, and if it is not, I label it as 0, which gives me a clear up or down classification target. I remove the last row because there is no next hour available for it, so its target would be missing. Finally, I do a quick check to make sure the target lines up with the prices and to see how balanced the classes are, and I can see that the classes are basically balacned, which is a good indicator."
      ],
      "metadata": {
        "id": "CXZnsHK0LNSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# feature engineering\n",
        "\n",
        "#lagged returns\n",
        "btc['lag_return_1h'] = btc['return_1h'].shift(1)\n",
        "btc['lag_return_2h'] = btc['return_1h'].shift(2)\n",
        "btc['lag_return_6h'] = btc['return_1h'].shift(6)\n",
        "\n",
        "#rolling averages\n",
        "btc['rolling_mean_6h'] = btc['Close'].rolling(window=6).mean()\n",
        "btc['rolling_mean_24h'] = btc['Close'].rolling(window=24).mean()\n",
        "\n",
        "#rolling volatility\n",
        "btc['rolling_std_6h'] = btc['Close'].rolling(window=6).std()\n",
        "btc['rolling_std_24h'] = btc['Close'].rolling(window=24).std()\n",
        "\n",
        "#time-of-day and day-of-week\n",
        "btc['hour'] = btc['datetime'].dt.hour\n",
        "btc['day_of_week'] = btc['datetime'].dt.dayofweek\n",
        "\n",
        "#drop rows with missing values\n",
        "btc = btc.dropna().reset_index(drop=True)\n",
        "\n",
        "btc.head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T22:18:39.851232Z",
          "iopub.execute_input": "2025-12-17T22:18:39.851635Z",
          "iopub.status.idle": "2025-12-17T22:18:39.905182Z",
          "shell.execute_reply.started": "2025-12-17T22:18:39.851609Z",
          "shell.execute_reply": "2025-12-17T22:18:39.904395Z"
        },
        "id": "-4WtYbmOLNSW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I am creating the main features that the models will actually learn from.\n",
        "\n",
        "I start by adding lagged returns so the model can see how the price has been behaving over the last few hours, which helps capture short term momentum or reversals using only past information.\n",
        "\n",
        "Then I add rolling averages of the closing price over 6 and 24 hours to give the model a sense of short term trends and whether the current price is relatively high or low compared to recent history. I also include rolling volatility over the same windows, which tells the model how stable or choppy the market has been, since price direction can behave differently in high versus low volatility periods.\n",
        "\n",
        "I also extract the hour of the day and day of the week from the datetime column to allow the model to pick up on any recurring time based patterns in trading behavior.\n",
        "\n",
        "Lastly I drop the rows at the start of the dataset where lagged and rolling features cannot be computed becaue they required past data so that the data going into the models is clean and complete."
      ],
      "metadata": {
        "id": "styO03q8LNSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#exponential moving averages\n",
        "btc['ema_12'] = btc['Close'].ewm(span=12, adjust=False).mean()\n",
        "btc['ema_26'] = btc['Close'].ewm(span=26, adjust=False).mean()\n",
        "\n",
        "#MACD = ema_12 - ema_26\n",
        "btc['macd'] = btc['ema_12'] - btc['ema_26']\n",
        "\n",
        "#relative strength index\n",
        "delta = btc['Close'].diff()\n",
        "gain = delta.where(delta > 0, 0)\n",
        "loss = -delta.where(delta < 0, 0)\n",
        "\n",
        "avg_gain = gain.rolling(window=14).mean()\n",
        "avg_loss = loss.rolling(window=14).mean()\n",
        "\n",
        "rs = avg_gain / avg_loss\n",
        "btc['rsi_14'] = 100 - (100 / (1 + rs))\n",
        "\n",
        "#drop rows with missing values\n",
        "btc = btc.dropna().reset_index(drop=True)\n",
        "\n",
        "#look at the new columns\n",
        "btc[['ema_12','ema_26','macd','rsi_14']].head()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T22:18:39.906026Z",
          "iopub.execute_input": "2025-12-17T22:18:39.906332Z",
          "iopub.status.idle": "2025-12-17T22:18:39.937028Z",
          "shell.execute_reply.started": "2025-12-17T22:18:39.906294Z",
          "shell.execute_reply": "2025-12-17T22:18:39.936234Z"
        },
        "id": "gxnrjICWLNSW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I am adding a set of technical indicators that are commonly used in trading to summarize trend and momentum in a more structured way.\n",
        "\n",
        "Exponential moving averages smooth the price by repeatedly averaging it over time while giving more weight to the most recent closing prices. I did this using ewm() on the closing price, which updates the average each hour while gradually discounting older prices. The 12 hour EMA reacts more quickly to new price changes, while the 26 hour EMA changes more slowly, so using both allows the model to compare short term price movement to the longer term trend.\n",
        "\n",
        "The MACD is the difference between these two EMAs, and it is widely used as a momentum indicator, where larger positive values suggest upward momentum and negative values suggest downward momentum.\n",
        "\n",
        "The relative strength index measures whether recent price movements have been driven more by upward moves or downward moves over a fixed window. I did this by separating positive and negative price changes, averaging them over the past 14 hours, and combining them into a single score between 0 and 100. Higher values indicate stronger recent buying pressure, while lower values indicate stronger selling pressure, which helps summarize short term market momentum in a way the model can use.\n",
        "\n",
        "These indicators are some of the things people who trade for a living look at, but I wated to include them with the hopes that the model can learn from them like traders do. I want my model to learn similar patterns directly from the data instead of relying on fixed rules or thresholds.\n",
        "\n",
        "Like lagged returns and the rolling features, all of these indicators require looking back several hours, the first few rows contain missing values, so I drop them to keep the dataset clean before modeling."
      ],
      "metadata": {
        "id": "K5yWS9yfLNSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#final set of features the model will use\n",
        "feature_cols = [\n",
        "    'Close', 'Volume', 'return_1h', 'lag_return_1h', 'lag_return_2h', 'lag_return_6h', 'rolling_mean_6h', 'rolling_mean_24h', 'rolling_std_6h',\n",
        "    'rolling_std_24h', 'hour', 'day_of_week', 'ema_12', 'ema_26', 'macd', 'rsi_14'\n",
        "]\n",
        "\n",
        "#feature matrix X and target vector y\n",
        "X = btc[feature_cols].copy()\n",
        "y = btc['target'].copy()\n",
        "\n",
        "#check shape anc alss balance\n",
        "X.shape, y.shape, y.value_counts(normalize=True)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T22:18:39.937893Z",
          "iopub.execute_input": "2025-12-17T22:18:39.938113Z",
          "iopub.status.idle": "2025-12-17T22:18:39.948356Z",
          "shell.execute_reply.started": "2025-12-17T22:18:39.938096Z",
          "shell.execute_reply": "2025-12-17T22:18:39.947482Z"
        },
        "id": "8uyYQzuwLNSW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I am formally defining the inputs and outputs for the supervised learning models that I will implement later.\n",
        "\n",
        "I list out all the engineered features I want the models to learn from, and then use this list to build the feature matrix X, which contains only those columns, and set y equal to the target variable I created earlier.\n",
        "\n",
        "Finally, I do a quick check of the shapes and class proportions to make sure the number of rows line up correctly and that the target is still roughly balanced before moving on."
      ],
      "metadata": {
        "id": "YB6JQQZFLNSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#time based train-test-split\n",
        "\n",
        "#first 80% = train, last 20% = test\n",
        "split_index = int(len(X) * 0.8)\n",
        "\n",
        "X_train = X.iloc[:split_index].copy()\n",
        "X_test  = X.iloc[split_index:].copy()\n",
        "\n",
        "y_train = y.iloc[:split_index].copy()\n",
        "y_test  = y.iloc[split_index:].copy()\n",
        "\n",
        "#check split sizes\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T22:18:39.949193Z",
          "iopub.execute_input": "2025-12-17T22:18:39.94949Z",
          "iopub.status.idle": "2025-12-17T22:18:39.975511Z",
          "shell.execute_reply.started": "2025-12-17T22:18:39.949464Z",
          "shell.execute_reply": "2025-12-17T22:18:39.974471Z"
        },
        "id": "paW2pMliLNSX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I am using train-test-split on the data.\n",
        "\n",
        "I take the first 80 percent of the observations as the training set and reserve the last 20 percent as the test set so the model is always trained on past data and evaluated on future data. I specifically did this because randomly splitting time series data would leak future information into training and give unrealistically good results.\n",
        "\n",
        "I then split both the feature matrix and the target variable using the same cutoff point and do a quick shape check to make sure everything lines up correctly before moving on to model training."
      ],
      "metadata": {
        "id": "x6qV08cDLNSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EDA\n",
        "\n",
        "print(\"Final btc shape:\", btc.shape)\n",
        "print(\"X shape:\", X.shape, \"| y shape:\", y.shape)\n",
        "\n",
        "print(\"\\nData types (features):\")\n",
        "display(X.dtypes.value_counts())\n",
        "\n",
        "print(\"\\nMissing values (top 10):\")\n",
        "display(btc.isna().sum().sort_values(ascending=False).head(10))\n",
        "\n",
        "print(\"\\nHourly return sanity check:\")\n",
        "display(btc['return_1h'].describe())\n",
        "\n",
        "print(\"\\nHourly return extreme quantiles:\")\n",
        "display(btc['return_1h'].quantile([0.001, 0.01, 0.5, 0.99, 0.999]))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T22:18:39.976708Z",
          "iopub.execute_input": "2025-12-17T22:18:39.977027Z",
          "iopub.status.idle": "2025-12-17T22:18:40.00996Z",
          "shell.execute_reply.started": "2025-12-17T22:18:39.977001Z",
          "shell.execute_reply": "2025-12-17T22:18:40.009257Z"
        },
        "id": "p-Pz6x7YLNSX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# EDA Visuals\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#price history over time plot\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(btc['datetime'], btc['Close'])\n",
        "plt.title(\"BTC Hourly Closing Price Over Time\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Close Price (USD)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "#distribution of hourly returns plot\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.histplot(btc['return_1h'], bins=50, kde=True)\n",
        "plt.title(\"Distribution of Hourly Returns\")\n",
        "plt.xlabel(\"Hourly Return\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "#correlation heatmap of features plot\n",
        "plt.figure(figsize=(14, 10))\n",
        "corr = btc[feature_cols].corr()\n",
        "sns.heatmap(corr, cmap='coolwarm', center=0)\n",
        "plt.title(\"Correlation Heatmap of Features\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T22:18:40.010898Z",
          "iopub.execute_input": "2025-12-17T22:18:40.011209Z",
          "iopub.status.idle": "2025-12-17T22:18:43.725656Z",
          "shell.execute_reply.started": "2025-12-17T22:18:40.01116Z",
          "shell.execute_reply": "2025-12-17T22:18:43.724872Z"
        },
        "id": "wA4CkXuiLNSX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2) Exploratory Data Analysis\n",
        "\n",
        "After cleaning and feature engineering, the dataset contains over seventeen thousand hourly observations and sixteen feature columns, along with a binary target variable. All feature columns are numeric, including price, volume, lagged returns, rolling statistics, technical indicators, and time-based variables such as hour of day and day of week. The target variable is also numeric and represents whether the next hour’s closing price increases or decreases.\n",
        "\n",
        "Because this is a classification problem, I first check whether the target variable is balanced. The distribution shows that the classes are nearly evenly split, with slightly more upward movements than downward ones, which means there is no strong class imbalance that would bias the models toward one outcome. I also check for missing values throughout the dataset. Missing values appear only as a result of lagged features, rolling windows, and technical indicators that require historical data. These rows are dropped so that the final dataset used for modeling is complete and consistent.\n",
        "\n",
        "To better understand the structure of the data, I examine several visual summaries. The price history plot shows that Bitcoin’s hourly closing price exhibits strong trends, sharp reversals, and periods of high volatility rather than stable behavior. This highlights why short-term prediction must rely on recent market conditions instead of long-term averages.\n",
        "\n",
        "The distribution of hourly returns is centered close to zero with heavy tails, meaning most price changes are small but occasional large jumps occur in both directions. This confirms that the prediction task is noisy and not so straightforward.\n",
        "\n",
        "Finally, the correlation heatmap shows that some features derived from similar ideas, such as rolling averages and exponential moving averages, are correlated, while many other features capture different aspects of market behavior. What I took this to mean is the feature set contains a mix of complementary signals rather than redundant information, making god to train my models from."
      ],
      "metadata": {
        "id": "wxqFfQEeLNSY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3) Evaluation Metric\n",
        "\n",
        "Because the target variable is a binary classification outcome indicating whether Bitcoin's price will increase in the next hour, I evaluate model performance using the F1 score. This metric is appropriate because it balances precision and recall, treating false positives and false negatives symmetrically.\n",
        "\n",
        "Although the target distribution is nearly perfectly balanced (approximately 50.9% up vs. 49.1% down), hourly crypto returns are noisy and small directional changes can be difficult to predict, making F1 a better measure of meaningful classification performance than accuracy alone.\n",
        "\n",
        "F1 ensures that the model must correctly identify both upward and downward movements rather than defaulting to the majority direction, and is better for my goal of detecting real directional shifts.\n"
      ],
      "metadata": {
        "id": "F2JIumAGLNSY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4) Model Fitting\n",
        "\n",
        "To evaluate different modeling approaches, I fit three supervised learning models.\n",
        "\n",
        "I chose logistic regression with regularization because it is a simple and interpretable baseline model.\n",
        "\n",
        "Random forest is included as a flexible, tree-based ensemble method that can capture nonlinear relationships and interactions better than my first model.\n",
        "\n",
        "Finally, I use a support vector machine with a nonlinear kernel to model more complex decision boundaries in a high-dimensional feature space.\n",
        "\n",
        "Using these three models allows for a meaningful comparison across different levels of model complexity while following the assignment requirement of drawing from distinct methodological chapters.\n",
        "\n",
        "All models are trained using the same time-based training set and tuned using cross-validation with TimeSeriesSplit to ensure that validation data always comes after training data in time. Hyperparameters are selected using cross-validation based on the F1 score, and the final tuned models are evaluated on a held-out test set."
      ],
      "metadata": {
        "id": "5nhF_9ckLNSY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Logistic Regression with L2 regularization\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "log_reg_pipe = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('logreg', LogisticRegression(max_iter=500))\n",
        "])\n",
        "\n",
        "param_grid = {\n",
        "    'logreg__C': [0.001, 0.01, 0.1, 1, 10]\n",
        "}\n",
        "\n",
        "log_reg_cv = GridSearchCV(\n",
        "    estimator=log_reg_pipe,\n",
        "    param_grid=param_grid,\n",
        "    cv=tscv,\n",
        "    scoring='f1',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "log_reg_cv.fit(X_train, y_train)\n",
        "\n",
        "best_logreg = log_reg_cv.best_estimator_\n",
        "best_logreg_C = log_reg_cv.best_params_['logreg__C']"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T22:18:43.726505Z",
          "iopub.execute_input": "2025-12-17T22:18:43.727026Z",
          "iopub.status.idle": "2025-12-17T22:18:46.556183Z",
          "shell.execute_reply.started": "2025-12-17T22:18:43.726996Z",
          "shell.execute_reply": "2025-12-17T22:18:46.555225Z"
        },
        "id": "dns0wPMeLNSY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "I use logistic regression as a baseline model because it provides a clear and interpretable starting point for a binary classification problem. Since the features operate on very different scales, I standardize them so the model and regularization behave correctly. I include L2 regularization to reduce overfitting in noisy hourly price data by shrinking coefficients rather than allowing the model to fit small fluctuations too closely. The regularization strength is tuned over a range of values to balance bias and variance. TimeSeriesSplit is used during cross-validation to avoid leaking future information, and the F1 score is used so the model must correctly identify both upward and downward price movements rather than defaulting to a single direction."
      ],
      "metadata": {
        "id": "3fCQD7p6LNSY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Random Forest Classifier with time-based CV\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
        "\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [100, 300],\n",
        "    'max_depth': [None, 5, 10],\n",
        "    'min_samples_leaf': [1, 5]\n",
        "}\n",
        "\n",
        "rf_cv = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid_rf,\n",
        "    cv=tscv,\n",
        "    scoring='f1',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "rf_cv.fit(X_train, y_train)\n",
        "\n",
        "best_rf = rf_cv.best_estimator_\n",
        "best_rf_params = rf_cv.best_params_"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T22:18:46.557229Z",
          "iopub.execute_input": "2025-12-17T22:18:46.557542Z",
          "iopub.status.idle": "2025-12-17T22:20:33.953293Z",
          "shell.execute_reply.started": "2025-12-17T22:18:46.55752Z",
          "shell.execute_reply": "2025-12-17T22:20:33.952459Z"
        },
        "id": "2yDMtEwqLNSZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "I use a random forest model to allow for nonlinear relationships and interactions between features that a linear model cannot capture. Random forests combine many decision trees to reduce variance and improve stability compared to a single tree. I tune the number of trees to balance predictive stability and computational cost, and I tune tree depth and minimum leaf size to control overfitting, which I thought would be important in high-frequency financial data like my dataset where noise is common. Cross-validation is again done using TimeSeriesSplit, and the F1 score is used for tuning so the model is evaluated consistently with the other approaches."
      ],
      "metadata": {
        "id": "Y5GWqgVlLNSZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#SVM\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "svm_pipe = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('svm', SVC())\n",
        "])\n",
        "\n",
        "param_grid_svm = {\n",
        "    'svm__C': [0.1, 1, 10],\n",
        "    'svm__kernel': ['rbf'],\n",
        "    'svm__gamma': ['scale', 0.01, 0.001]\n",
        "}\n",
        "\n",
        "svm_cv = GridSearchCV(\n",
        "    estimator=svm_pipe,\n",
        "    param_grid=param_grid_svm,\n",
        "    cv=tscv,\n",
        "    scoring='f1',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "svm_cv.fit(X_train, y_train)\n",
        "\n",
        "best_svm = svm_cv.best_estimator_\n",
        "best_svm_params = svm_cv.best_params_"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T22:20:33.954297Z",
          "iopub.execute_input": "2025-12-17T22:20:33.954529Z",
          "iopub.status.idle": "2025-12-17T22:22:06.291913Z",
          "shell.execute_reply.started": "2025-12-17T22:20:33.95451Z",
          "shell.execute_reply": "2025-12-17T22:22:06.291105Z"
        },
        "id": "wPFV9DpiLNSa"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "I include a support vector machine with a radial basis function kernel because it is well suited for high-dimensional data and can model complex, nonlinear decision boundaries. Since SVMs are sensitive to feature scale, I standardize the inputs using a pipeline to ensure stable optimization. The regularization parameter is tuned to control how strictly the model fits the training data, while the kernel parameter is tuned to adjust how locally the model responds to individual observations. As with the other models, time-based cross-validation is used to prevent leakage, and the F1 score is used to select the final model configuration. This model provides a contrast to both the linear baseline and the tree-based ensemble in terms of flexibility and generalization."
      ],
      "metadata": {
        "id": "9-CssApWLNSa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5) Model Comparison"
      ],
      "metadata": {
        "id": "VPLkcVtCLNSa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "#Logistic Regression\n",
        "y_pred_logreg = best_logreg.predict(X_test)\n",
        "f1_logreg = f1_score(y_test, y_pred_logreg)\n",
        "\n",
        "#Random Forest\n",
        "y_pred_rf = best_rf.predict(X_test)\n",
        "f1_rf = f1_score(y_test, y_pred_rf)\n",
        "\n",
        "#SVM\n",
        "y_pred_svm = best_svm.predict(X_test)\n",
        "f1_svm = f1_score(y_test, y_pred_svm)\n",
        "\n",
        "#results\n",
        "model_scores = {\n",
        "    \"Logistic Regression (L2)\": f1_logreg,\n",
        "    \"Random Forest\": f1_rf,\n",
        "    \"SVM (RBF Kernel)\": f1_svm\n",
        "}\n",
        "\n",
        "model_scores"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T22:22:06.292836Z",
          "iopub.execute_input": "2025-12-17T22:22:06.293126Z",
          "iopub.status.idle": "2025-12-17T22:22:08.89641Z",
          "shell.execute_reply.started": "2025-12-17T22:22:06.293088Z",
          "shell.execute_reply": "2025-12-17T22:22:08.895482Z"
        },
        "id": "JfKiz4XRLNSz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The three models show clear differences in performance and behavior when evaluated on the held-out test set. The support vector machine achieves the highest F1 score, followed by logistic regression, while the random forest performs the worst.\n",
        "\n",
        "Logistic regression has the highest bias and lowest variance among the three models, which limits its flexibility but makes it stable and easy to interpret. Its performance suggests that linear relationships capture some useful signal in the data, but not enough to fully model the complexity of short-term Bitcoin price movements.\n",
        "\n",
        "Random forest is much more flexible and has lower bias, but this flexibility comes with higher variance, and in this setting it appears to overfit noise in the training data, leading to weaker generalization on the test set.\n",
        "\n",
        "The support vector machine strikes the best balance between bias and variance by allowing nonlinear decision boundaries while still enforcing strong regularization, which helps it generalize better than the other models.\n",
        "\n",
        "In terms of interpretability, logistic regression is the easiest to understand, random forest offers some interpretability through feature importance but is harder to analyze directly, and the SVM is the least interpretable.\n",
        "\n",
        "Overall, the results suggest that moderate flexibility with strong regularization is most effective for this task, which explains why the SVM performs best on out-of-sample data."
      ],
      "metadata": {
        "id": "A2ER9fKgLNSz"
      }
    }
  ]
}